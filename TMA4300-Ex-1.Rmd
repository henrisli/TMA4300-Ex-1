--- 
title: 'TMA4300 Computer Intensive Statistical Methods Exercise 1, Spring 2019'
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Henrik Syversveen Lie, Mikal Stapnes (SKRIVE STUDENTNUMMER I STEDET??)'
---


```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```

# Problem A: Stochastic simulation by the probability integral transform and bivariate techniques

## 1.
We make a function that draws $n$ random numbers from an exponential distribution with parameter $\lambda$. If $u$ is uniformly distributed between 0 and 1, $u\sim U(0,1)$, then $x=-\log(u)/\lambda$ will be exponentially distributed with parameter $\lambda$, or $x\sim \text{Exp}(\lambda)$.
```{r, echo = T, eval = T}
library(ggplot2)
exponential<- function(lambda, n=1){
  u = runif(n)
  x = -1/lambda*log(u)
  return(x)
}
```
Then we want to check if this function is correct. To do this, we check a few things. Firstly we compare the mean and variance of a sample drawn from our function with the theoretical mean and variance. The theoretical mean and variance of a exponential distribution are $\mu = 1/\lambda$ and $\sigma^2 = 1/\lambda^2$. We draw a sample with $\lambda = 4.300$ and compare:
```{r, echo = T, eval = T}
n = 10000
lambda = 4.300
x = exponential(lambda,n)
cat('Theoretical mean: ', 1/lambda,' Computed mean: ', mean(x),'\n')
cat('Theoretical variance: ', 1/lambda^2, ' Computed variance: ', var(x))
```
We see that they are almost the same. Finally we make two plots. The first is a histogram comparison of our function and the R function `rexp()`, which draws random samples from an exponential distribution. The second plot is a histogram of our random sample compared with the analytical exponential distribution.
```{r, echo = T, eval = T}
y = rexp(n, lambda)
rfunc = data.frame(value = y)
ourfunc = data.frame(value = x)
rfunc$type = 'R function'
ourfunc$type = 'Our function'
df = rbind(rfunc,ourfunc)
#ggplot(df, aes(value, fill = type)) + 
#   geom_histogram(alpha = 0.5, aes(y = ..density..), binwidth = 0.05, position = 'identity') + ggtitle("Our function vs. R function densities")
df2 <- data.frame(theo=seq(0,max(x),length.out=n), value=x)

ggplot(df2, aes(x = value)) + geom_histogram(aes(y = ..density..), binwidth=0.05, fill = 'blue', color = 'blue') + stat_function(fun=dexp,geom = "line",size=1.6,col="red",args = (mean=lambda)) + ggtitle("The exponential pdf - comparison of analytical and sampled density") + labs(x='x', y='density', caption='Comparison of the analytical density of the exponential distribution (red line) and the sampled density (blue bars).')
```

From the figure we see that the density of our samples overlaps with the analytical density and can thus conclude that our algorithm is correct. 

## 2.

We wish to sample from the pdf 

$$g(x) = \begin{cases} c x^{\alpha - 1}, \quad 0 < x < 1, \\
c e^{-x}, \quad 1 \leq x, \\
0, \quad otherwise.
\end{cases}
$$

We first note that the normalizing constant will be $c=\frac{e\alpha}{e+\alpha}$, which can be found by solving $$\int_0^{\infty} g(x)dx = 1.$$

We find the cumulative distribution by noting that $G_X(x) = P(X\leq x)$, which gives 
$$G_X(x) = \int_0^x g(\xi)d\xi = \begin{cases} \int_0^x c\xi^{\alpha-1}d\xi = \frac{cx^{\alpha}}{\alpha}, \quad &0\leq x\leq 1,\\ \frac{c}{\alpha}+\int_1^x c\exp(-\xi)d\xi = \frac{c}{\alpha}+\frac{c}{e}-c\exp(-x), \quad &1\leq x,\\0, &\text{otherwise}.\end{cases}$$
Because $c=\frac{e\alpha}{e+\alpha}$, we have that $c/\alpha + c/e=1$, giving

$$G_X(x) = \begin{cases} \frac{cx^{\alpha}}{\alpha}, \quad &0\leq x\leq 1,\\ 1-c\exp(-x), \quad &1\leq x,\\0, &\text{otherwise}.\end{cases}$$
Then we take the inverse of this function to obtain
$$G_X^{-1}(u) = \begin{cases} (\frac{\alpha}{c}u)^{1/\alpha}, \quad &0 \leq u<\frac{e}{\alpha+e}, \\
\ln(\frac{c}{1-u}), \quad &\frac{e}{\alpha+e} \leq u \leq 1.
\end{cases}$$

This can be used to sample from $g(x)$. To confirm this, we sample from $u\sim U(0,1)$ and compute the inverse $X = G^{-1}_X(u)$. Then our claim is that $X \sim g(x)$. We start by computing the analytical mean of our distribution and compare this with the mean of the drawn sample. We find the mean by
$$\text{E}(X) = \int_0^{\infty} xg(x)dx = \frac{c}{\alpha+1} + \frac{2c}{e}.$$
We compare results in R.

```{r, echo = T, eval = T}
g = function(x, alpha){
  c = alpha*exp(1)/(alpha+exp(1))
  result = vector(length = length(x))
  result[x<1.] = sapply(x[x<1.], function(x) c*x^(alpha - 1))
  result[x>=1.] = sapply(x[x>=1.], function(x) c*exp(-x))
  return(as.double(result))
}

G_inv = function(n, alpha){
  c = alpha*exp(1)/(alpha+exp(1))
  u = runif(n)
  result = vector(length = n)
  result[u < c/alpha] = (alpha/c * u[u < c/alpha])^(1/alpha)
  result[u >= c/alpha] = -log(1/alpha + exp(-1) - u[u >= c/alpha]/c)
  return(result)
}

alpha = 2.5
n = 10000
xsamples = G_inv(n, alpha)
c = alpha*exp(1)/(alpha+exp(1))
mean = c/(alpha+1) + 2*c/exp(1)
cat('Theoretical mean: ', mean, ' Sampled mean: ', mean(xsamples))
```

Then we compute the theoretical variance by $\text{Var}(X) = \text{E}(X^2)-\text{E}(X)^2$ and compare it with the sampled variance. The theoretical variance will be
$$\text{Var}(X) = \text{E}(X^2) - \text{E}(X)^2 = \frac{c}{\alpha+2}+\frac{5c}{e} - \big(\frac{c}{\alpha+1} + \frac{2c}{e}\big)^2.$$

We compare results in R
```{r, echo = T, eval = T}
variance = c/(alpha+2)+5*c/exp(1) - mean^2
cat('Theoretical variance: ', variance, ' Sampled variance: ', var(xsamples))
```

We see that the sampled mean and variance coincide with the theoretical mean and variance.

Finally we plot a histogram of the sample and compare it to the density function $g(x)$.

```{r, echo = T, eval = T}
df2 <- data.frame(theo=seq(0,max(xsamples),length.out=n), value=xsamples)
ggplot(df2, aes(x = value)) + geom_histogram(aes(y = ..density.., color='Sampled'), binwidth=0.1) + stat_function(fun=g,geom = "line",size=1., args = (mean=alpha), alpha=1., aes(color="Analytical")) + ggtitle("g(x), Comparison of analytical and sampled density") + labs(x='x', y='density', caption=paste('Comparison of the analytical density of g(x) (red line) \n and the sampled density (blue bars).'))
```

From the figure we see that the analytical density overlaps with the sampled density, confirming the our algorithm is correct.

## 3.

To simulate from the standard normal distribution, we implement a function that performs the Box-Müller transformation. Given $x_1 \sim U(0,1)$ and $x_2 \sim \text{Exp}(1/2)$, we have that $y_1 = \sqrt{x_2}\cos{(2\pi x_1)}$ and $y_2 = \sqrt{x_2}\sin{(2\pi x_1)}$ will be i.i.d. standard normal. To sample from the exponential distribution, we use the `exponential` function implemented earlier. The following function implements the Box-Müller transformation to generate $n$ independent samples from the standard normal distribution. To reduce runtime of the algorithm, we assume $n$ to be an even number, so that we can generate only $n/2$ samples from the uniform and exponential distributions and use both $y_1$ and $y_2$.
```{r, echo = T, eval = T}
normal <- function(n){
  x_1 <- runif(n/2)
  x_2 <- exponential(1/2, n/2)
  y_1 <- sqrt(x_2)*cos(2*pi*x_1)
  y_2 <- sqrt(x_2)*sin(2*pi*x_1)
  y <- c(y_1,y_2)
  return(y)
}
```

Then we want to verify the algorithm, so we compute the mean and variance of the sample.
```{r, echo = T, eval = T}
n = 100000
x <- normal(n)
cat('Theoretical mean: ', 0,' Computed mean: ', mean(x),'\n')
cat('Theoretical variance: ', 1, ' Computed variance: ', var(x))
```
We see that the computed mean and variance coincide with the theoretical mean and variance of 0 and 1. 

Finally we make a histogram of our sample and compare it with the analytical density of the standard normal distribution. In addition, we provide a Q-Q plot to check normality of the data.

```{r, echo = T, eval = T}
df2 <- data.frame(theo=seq(0,max(x),length.out=n), value=x)
ggplot(df2, aes(x = value)) + geom_histogram(aes(y = ..density..), binwidth=0.1, col = 'blue', fill = 'blue') + stat_function(fun=dnorm,geom = "line",size=1.6,col="red",args = list(mean=0, sd = 1)) + ggtitle("The normal pdf - comparison of analytical and sampled density") + labs(x='x', y='density', caption=paste('Comparison of the analytical density of the Normal(0, 1) distribution (red line) \n and the sampled density (blue bars).'))

qqnorm(x, pch = 1, frame = FALSE)
qqline(x, col = "steelblue", lwd = 2)
```

Also here, the histogram coincides well with the standard normal density function, so we conclude that our algorithm is correct. Lastly, the sampled points in the Q-Q plot coincide with the analytical line.

## 4.
We now implement a function that generates samples from a $d$-variate normal distribution with mean vector $\mu$ and covariance matrix $\Sigma$. We use the function `normal`, which we created in task A.3. to generate $d$ i.i.d. standard normal samples. Then we use the transformation $y = \mu + Ax \sim N(\mu, A A^T)$, with $AA^T=\Sigma$ and $x$ being a vector with the independent standard normal samples. We use the Cholesky decomposition for A.
```{r, echo = T, eval = T}
normal_d <- function(d, mu, sigma){
  x <- normal(d)
  y <- mu + t(chol(sigma))%*%x
  return(y)
}
```

Then, we draw samples from a $d$-variate distribution to check our results. We use
$$\mu = \begin{pmatrix} 1 \\ 2 \end{pmatrix}, \quad \Sigma = \begin{pmatrix} 2.5 & 3.0 \\ 3.0 & 10.0\end{pmatrix}.$$
We check if the generated sample has the correct mean vector and covariance matrix to see if the generated sample truly is $d$-variate normal.
```{r, echo = T, eval = T}
d = 2
n = 100000
mu = c(1,2)
sigma = matrix(c(2.5,3,3,10),ncol=2,nrow=2)
sample = matrix(NA, ncol = d, nrow = n)
for (i in 1:n){
  sample[i,] <- normal_d(d,mu,sigma)
}
average = apply(sample, 2,mean)
cat('Mean vector: \n')
average
covmat = cov(sample)
cat('Covariance matrix: \n')
covmat
```
Both the mean vector and the covariance matrix coincide well with the theoretical mean and covariance.

Finally we implement an Anderson-Darling normality test to check the normality of the data. We check that each of the two elements are normal.
```{r, echo = T, eval = T}
library(nortest)
ad.test(sample[,1])
ad.test(sample[,2])
```
Both p-values are above any reasonable significance level, which does not reject the hypothesis that the data is normal distributed, and we conclude that our algorithm is correct.

# Problem B: The gamma distribution

## 1.

We can sample from the Gamma distribution with $\alpha \in (0, 1)$ and $\beta = 1$,

$$ f(x) = \begin{cases}
\frac{1}{\Gamma(\alpha)}x^{\alpha-1}e^{-x}, \quad & 0<x,\\
0, \quad &\text{otherwise}, \\
\end{cases} $$

using rejection sampling with the distribution $g(x)$ found in A.2 as a proposal distribution. We can do this because $g(x)$ is nonzero at all points where $f(x)$ is nonzero. 

We note that the acceptance probabiltiy, $P_{accept}$, 

$$P_{accept} = \int_0^{\infty} \frac{f(x)}{c g(x)} g(x)dx = c^{-1}.$$
 
To maximize this acceptance probability, we can minimize $c$ by choosing

$$ c = \underset{x}{\sup} \frac{f(x)}{g(x)} = \underset{x}{\sup} \begin{cases}
\frac{e^{-x}(\frac{1}{\alpha} + \frac{1}{e})}{\Gamma(\alpha)}, \quad 0 \leq x \le 1 \\
\frac{x^{\alpha - 1} (\frac{1}{\alpha} + \frac{1}{e})}{\Gamma(\alpha)}, \quad 1 \leq x
\end{cases}$$ 

Which means $c = \frac{(\frac{1}{\alpha} + \frac{1}{e})}{\Gamma(\alpha)}$

To generate $n$ samples from $f(x)$ we sample from $X \sim g(x)$ and $U \sim u[0, 1]$, and accept the sample $x$ if $u \leq \frac{f(x)}{c g(x)}$. We repeat the process until $n$ samples are accepted. 

```{r, echo = T, eval = T}
small_gamma = function(alpha, n=1){
  result = vector()
  iter = 0
  sampled = 0
  c = 1/(gamma(alpha)) * (1/alpha + exp(-1))
  
  while(sampled < n){

    #sample from g(x)
    uniform2 = runif(n)
    x = G_inv(n, alpha)
    #compute the acceptance probabilites
    probs = dgamma(x, alpha, rate=1) / (c * as.double(g(x, alpha)))
    #append samples where u < f(x) / c g(x)
    result = append(result, x[uniform2 <= probs])
    #increase counters
    sampled = sampled + sum(uniform2 <= probs)
    iter = iter + n
  }
  
  iter = iter - length(result) + n
  
  return(list(samples = result[seq(1:n)], iter = iter))
}
```

We compare the sample mean and variance with the theoretical mean and variance and compare the sampled density with the analytical density. 

```{r, echo = T, eval = T}
alpha = 0.95
G1 <- small_gamma(alpha, 10000)
df12 = data.frame(G1)

cat('Theoretical mean: ', alpha,' Computed mean: ', mean(G1$samples),'\n')
cat('Theoretical variance: ', alpha, ' Computed variance: ', var(G1$samples))
```

We see that the computed mean and variance coincide with the theoretical mean and variance.

```{r, echo = T, eval = T}
ggplot(data=df12, aes(x=samples)) + geom_histogram(data=df12, aes(x=samples, y=..density.., color="Sampled"), binwidth=0.1) +  stat_function(fun=dgamma, geom = "line",size=1.6, args = list(shape=alpha, rate=1), aes(x=samples, color = "Analytical")) + ggtitle("The Gamma pdf - comparison of analytical and sampled density for small alpha") + ylim(0. ,2.0) + labs(x='x', y='density', caption=paste('Comparison of the analytical density of the Gamma(', alpha, ',1) distribution (red line) \n and the sampled density (blue bars).'))

```

From the figure we see that we get the correct distribution when using rejection sampling. 

## 2.

As we remove the limitation on $\alpha$, we no longer get a closed form expression for the $c$ giving the highest acceptance probability. Instead, we may use the ratio of uniforms method to sample from the Gamma with parameters $\alpha > 1$, $\beta = 1$. We define the area

$$C_f = \bigg \{(x_1, x_2): 0 \leq x_1 \leq \sqrt{f^*(\frac{x_2}{x_1})} \bigg \}, \quad f^*(x) = \begin{cases}
x^{\alpha-1}e^{-x}, \quad 0 < x, \\
0, \quad \text{otherwise}.
\end{cases}$$ 

And note that $y = x_2 / x_1$ will be Gamma-distributed if sampled uniformly inside $C_f$. By finding 

$$ a = \sqrt{(\underset{x}{\sup} f^* (x)}, \quad b_{+} = \sqrt{ \underset{x\geq 0}{\sup} x^2 f^* (x)} , \quad b_{\_} = - \sqrt{\underset{x \leq 0}{\sup} x^2 f^* (x)}$$


$$ (f^*(x))' = 0 \Leftrightarrow x^{\alpha-2}e^{-x}(\alpha - 1  - x) = 0 \Rightarrow x = \alpha - 1 \\
a = \sqrt{(\alpha - 1)^{\alpha - 1}e^{1-\alpha}}$$

$$(x^2 f^*(x))' = 0 \Leftrightarrow x^{\alpha}e^{-x}(\alpha + 1 - x) = 0 \\
b_{+} = \sqrt{(\alpha + 1)^{\alpha + 1}e^{-\alpha - 1}}$$

$$b_{-} = 0$$

We can sample uniformly from $[0, a]\times[b_{-}, b_{+}]$ and select the set of samples that are also in $C_f$. Then $y = \frac{x_2}{x_1}$ will be Gamma($\alpha ,1$) distributed. Note that in the code below, we check if $(x_1, x_2) \in C_f$ on a log-scale as $\log a, \log b$ are more easily stored than $a, b$ when $\alpha$ become large. The equivalent condition becomes
$$\log x_1 = \log a + \log u_1, \quad u_1 \sim U[0, 1]\\
\log x_2 = \log b + \log u_2, \quad u_2 \sim U[0, 1] \\
(x_1, x_2) \in C_f \Leftrightarrow 2 \log x_1 \leq (\alpha-1)(\log x_2 - \log x_1) - \exp\{\log x_2 - \log x_1\}$$.

```{r, echo = T, eval = T}
large_gamma <- function(alpha, n=1){
  #log transformations of a and b+
  loga = 1/2 * (alpha - 1) * (log(alpha - 1) - 1)
  logb = 1/2 * (alpha + 1) * (log(alpha + 1) - 1)
  result = vector(mode="numeric")
  
  i = 0
  iter = 0
  while (i < n){
    iter = iter + 1 
    log_x_1 = loga + log(runif(1))
    log_x_2 = logb + log(runif(1))
    #equivalent condition of (x_1, x_2) in C_f
    if (2*log_x_1 <= (alpha - 1)*(log_x_2 - log_x_1) - exp(log_x_2 - log_x_1)){
      result = append(result, exp(log_x_2 - log_x_1))
      i = i + 1
    }
    else{}
  }
  list(samples = result, iter = iter)
}
```

```{r, echo = T, eval = T}
alpha = 7.
G2 = large_gamma(alpha, 10000)
cat('Theoretical mean: ', alpha,' Computed mean: ', mean(G2$samples),'\n')
cat('Theoretical variance: ', alpha, ' Computed variance: ', var(G2$samples))
```

We see that the computed mean and variance coincide with the theoretical mean and variance.

```{r, echo = T, eval = T}
df16 = data.frame(data=G2)

ggplot(df16, aes(x=data.samples)) + geom_histogram(data=df16, aes(x = data.samples, y=..density.., color="Sampled Function"), binwidth=0.2) + stat_function(fun=dgamma,geom = "line",size=1.6,aes(color='Analytical'),args = list(shape=alpha, rate = 1)) + ggtitle("The Gamma pdf - comparison of analytical and sampled density for large alpha") + labs(x='x', y='density', caption=paste('Comparison of the analytical density of the Gamma(', alpha, ',1) distribution (red line) \n and the sampled density (blue bars).'))
```

From the figure we see that we get the correct distribution when sampling using the ratio of uniforms method.

We wish to investigate in what manner $\alpha$ affects the acceptance probability $P$ of the ratio of uniforms method. We draw $n = 1000$ samples for $\alpha \in (1, 2000]$ and count, for each $\alpha$, the average number of iterations of the rejection sampling algorithm required to sample 1 gamma-distributed variable. The inverse of this will then be an estimate of the acceptance probability. 
```{r, echo = T, eval = T}
iterations = c()

for (alpha in seq(2, 2000, 10)){
  #Sample 1000 gamma variables for a particular alpha
  G = large_gamma(alpha, 1000)
  #Iter is here the average over all 1000 samples
  iterations = append(iterations, G$iter)
}

df15 = data.frame(alpha = seq(5, 2000, 10), P_estimate = 1000/iterations)

ggplot(df15, aes(x = alpha, y = P_estimate)) + geom_line() + ggtitle("Using ratio of uniforms to sample from the Gamma - acceptance probability") + labs(caption='Estimated acceptance probability as a function of alpha')
```
From the plot we see that $P_accept$ decreases with larger $\alpha$ but seems to reach a lower bound as $\alpha$ approaches $2000$. This is as expected; with increasing $\alpha$, the pdf flattens out and the proportion of the area in $[0, a]\times[b_{-}, b_{+}]$ covered by $C_{f}$ is decreased. 

We know have reasonably efficient algorithms for sampling from the Gamma with $\alpha \in (0, 1), \beta = 1$ and $\alpha \in (1, \infty), \beta = 1$. We note that for $\alpha = 1, \beta = 1$, we observe that the pdf of the gamma coincides with the unscaled exponential distribution. We also note that $\beta$ is an inverse scale parameter, and $\frac{1}{\beta}X \sim Gamma(\alpha, \beta)$ if $X \sim (\alpha, 1)$. We use this and our two algorithms to develop a general algorithm for sampling from the gamma. 

```{r, echo = T, eval = T}
gamma2 = function(alpha, beta=1, n  = 1){
  if (alpha == 1.0){
    return(list(samples = exponential(beta, n), iter = n))
  }
  else if (alpha <= 1){
    G = small_gamma(alpha, n)
    G$samples = G$samples / beta
    return(G)
  }
  else if (alpha > 1){
    G = large_gamma(alpha, n)
    G$samples = G$samples / beta
    return(G)
  }
}

alpha = 1000
beta = 1

G = gamma2(alpha, beta, 10000)

df17 <- data.frame(G)

histo = hist(G$samples, plot=F)
ggplot(df17, aes(x = samples)) + geom_histogram(aes(y = ..density.., color='Sampled'), bins=50) + stat_function(fun=dgamma,geom = "line",size=1.6,aes(color='Analytical'),args = list(shape=alpha, rate=beta)) + ggtitle("The Gamma pdf - comparison of analytical and sampled density for extreme alpha") + ylim(0, 1.1*max(histo$density)) + labs(x='x', y='density', caption=paste('Comparison of the analytical density of the Gamma(', alpha, ',1) distribution (red line) \n and the sampled density (blue bars).'))
```
We note that as $\alpha \rightarrow \infty$, as in the figure above, the Gamma pdf approaches a normal pdf as $\alpha \rightarrow \infty$. Taking advantage of this, sophisticated sampling techniques modify samples from the normal when sampling from the Gamma with very large $\alpha$. 


# Problem C: The Dirichlet distribution: simulation using known relations

## 1.
We assume $z_k\sim \Gamma(\alpha_k,1)$ for $k=1,\ldots,K$ independently, and define $x_k = z_k/(z_1+\dots+z_K)$ for $k=1,\ldots,K$. Then we want to find the distribution of $x= (x_1,\ldots,x_K)$. We use a transformation of the joint distribution of $z = (z_1,\ldots,z_K)$ to $(x_1,\ldots,x_{K-1},v), v = z_1+\dots+z_K$. The inverse transformation will be $z_k = vx_k, k = 1,\ldots,K$, which has a Jacobian
$$\lvert J \rvert = \begin{vmatrix}\frac{\partial z_1}{\partial x_1}  & \dots & \frac{\partial z_1}{\partial x_{K-1}} & \frac{\partial z_1}{\partial v}\\ \vdots &  \ddots & \vdots & \vdots \\ \frac{\partial z_K}{\partial x_1} & \dots & \frac{\partial z_K}{\partial x_{K-1}}&\frac{\partial z_K}{\partial v} \end{vmatrix} = \begin{vmatrix}v&0  & \dots & 0 & x_1\\ 0&v&\dots & 0 & x_2\\\vdots &\vdots&  \ddots & \vdots & \vdots \\0 & 0& \dots&v&x_{K-1}\\ -v &-v& \dots & -v&1-\sum_{k=1}^{K-1}x_k  \end{vmatrix} = v^{K-1}.$$
Then, we note that the joint distribution of $z_k, k = 1,\ldots,K$ will be
$$F_{z_1,\ldots,z_K}(z_1,\ldots,z_K) = \prod_{k=1}^K \frac{1}{\Gamma(\alpha_k)}z_k^{\alpha_k-1}\exp(-z_k)=\prod_{k=1}^K \bigg(\frac{1}{\Gamma(\alpha_k)}z_k^{\alpha_k-1}\bigg)\exp(-\sum_{k=1}^K z_k).$$
Now, we do the transformation with $z_k = vx_k, k = 1,\ldots,K-1$ and $v = z_1+\dots+z_K$ to get
\begin{align*}F_{x_1,\ldots,x_{K-1},v}(x_1,\ldots,x_{K-1},v) &= \prod_{k=1}^{K-1} \bigg(\frac{1}{\Gamma(\alpha_k)}(vx_k)^{\alpha_k-1}\bigg)\cdot\frac{1}{\Gamma(\alpha_K)}(v\big(1-\sum_{k=1}^{K-1}x_k\big))^{\alpha_K-1}\exp(-v)v^{K-1}\\
&= \prod_{k=1}^{K}\bigg(\frac{1}{\Gamma(\alpha_k)}\bigg)v^{\big(\sum_{k=1}^K\alpha_k\big)-1}\exp(-v)\bigg(\prod_{k=1}^{K-1}x_k^{\alpha_k-1}\bigg)\bigg(1-\sum_{k=1}^{K-1}x_k\bigg)^{\alpha_K-1}.
\end{align*}

Finally, we want to find the marginal distribution of $F_{x_1,\ldots,x_{K-1}}(x_1,\ldots,x_{K-1})$, so we integrate out $v$. The variable $v=z_1+\dots+z_k$ is a sum of Gamma distributed variables, so it can vary between 0 and $\infty$. We note that the Gamma function is defined by
$$\Gamma(z) = \int_0^\infty \exp(-x)x^{z-1}dx,$$
which gives $$\int_0^\infty \exp(-v)v^{\big(\sum_{k=1}^K\alpha_k\big)-1}dv = \Gamma(\sum_{k=1}^K\alpha_k),$$
and in total we have
$$ F_{x_1,\ldots,x_{K-1}}(x_1,\ldots,x_{K-1})=\bigg(\frac{\Gamma(\sum_{k=1}^K\alpha_k)}{\prod_{k=1}^{K}\Gamma(\alpha_k)}\bigg)\bigg(\prod_{k=1}^{K-1}x_k^{\alpha_k-1}\bigg)\bigg(1-\sum_{k=1}^{K-1}x_k\bigg)^{\alpha_K-1}.$$
We also know that $\sum_{k=1}^Kx_k=1$, $x_1,\ldots,x_{K-1}>0$ and $\sum_{k=1}^{K-1}x_k<1$, which means that $x=(x_1,\ldots,x_K)$ will be Dirichlet distributed.

## 2.
Now we want to implement a function that draws one realization from the Dirichlet distribution with parameter vector $(\alpha_1,\ldots,\alpha_K)$. We do so in the following code.
```{r, echo = T, eval = T}
library(MCMCpack)
get_gamma_samples <- function(alpha){
  return(gamma2(alpha)$samples)
}

dirichlet <- function(alpha){
  z<- sapply(alpha,get_gamma_samples)
  x <- z/sum(z)
  return(x)
}
```
We want to test if the function draws from the correct distribution. To do so, we first check the mean and variance of a generated sample. From theory, it is known that if $(x_1,\ldots,x_K)\sim \text{Dirichlet}(\alpha_1,\ldots,\alpha_K)$, then
$$\text{E}(X_k) = \frac{\alpha_k}{\alpha_0}, \quad \text{Var}(X_i) = \frac{\alpha_i(\alpha_0-\alpha_i)}{\alpha_0^2(\alpha_0+1)}, \quad \alpha_0 = \sum_{i=1}^K \alpha_i.$$
To determine $(\alpha_1,\ldots,\alpha_k)$, we simulate them from an exponential distribution with $\lambda = 1$ to ensure that all $\alpha$ are positive. We then compare sample mean and variance with theoretical mean and variance for a sample with $k=5$.
```{r, echo = T, eval = T}
k = 5
n = 20000
x <- matrix(NA, ncol = k, nrow = n)
alpha <- exponential(1,k)
for (i in 1:n){
  x[i,] <- dirichlet(alpha)
}
cat('Theoretical mean: ', alpha/sum(alpha),'\n','Computed mean: ', apply(x,2,mean),'\n')
cat('Theoretical variance: ', alpha*(sum(alpha)-alpha)/(sum(alpha)^2*(sum(alpha)+1)),'\n', 'Computed variance: ', apply(x,2,var))
```
We see that the mean and variance coincide well for all $x_k$. To further confirm that our implementation is correct, we compare a sample drawn from our function with the already implemented R function `rdirichlet` from the `MCMCpack` library. For simplicity, we let all $\alpha_k=1$, so each $x_k$ has the same marginal distribution. We then make a histogram of all $x_k$ from the two samples.

```{r, echo = T, eval = T}
n <- 10000
alpha <- rep(1,k)
x <- matrix(NA, ncol = k, nrow = n)
for (i in 1:n){
  x[i,] <- dirichlet(alpha)
}
y <- as.vector(rdirichlet(n,alpha))
rfunc = data.frame(value = y)
ourfunc = data.frame(value = as.vector(x))
rfunc$type = 'R function'
ourfunc$type = 'Our function'
df = rbind(rfunc,ourfunc)
ggplot(df, aes(value, fill = type)) + 
   geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity') + ggtitle('Comparison of Dirichlet samplers') + labs(x='x', y='density', caption=paste('Comparison of the sampled density of x_k\n sampled from our dirichlet sampler (red bars) and the sampler from MCMpack (blue bars)'))
```
Also here, the histograms coincide well. As a final test, we plot a histogram of our generated sample and compare it with the analytical density function. Because the Dirichlet distribution is a multivariate distribution, we only investigate the marginal distribution of $x_1$, and it can be shown that the marginal distribution of any $x_k$ is the Beta distribution with parameters $\alpha_k$ and $\sum_{i\neq k}\alpha_i$. This means that we can plot the density of $x_1$ from our generated sample and compare it with a Beta distribution density. Again, for simplicity we assume all $\alpha_k=1$.

```{r, echo = T, eval = T}
df2 <- data.frame(theo=seq(0,max(x[,1]),length.out=n), value=x[,1])
ggplot(df2, aes(x = value)) + geom_histogram(aes(y = ..density..), 
      binwidth = 0.005,fill = 'blue', color = 'blue') + stat_function(fun=dbeta,geom = "line",size=1.6,col="red",args = list(shape1=1,shape2=k-1)) + ggtitle("Our function vs. analytical density") + labs(x='x', y='density', caption=paste('Comparison of the sampled density of x_1 sampled from our dirichlet sampler (blue bars) and the Beta distribution (red line)'))
```

The histogram fits the analytical density well, and we conclude that we have implemented the function to generate Dirichlet samples correctly.

# Problem D: Rejection sampling and importance sampling

## 1.

We wish to sample from 

$$ f(\theta \lvert y) \propto (2 + \theta)^{y_1}(1-\theta)^{y_2 + y_3}\theta^{y_4}, \quad \theta \in (0, 1)$$

We construct a rejection sampling algorithm to sample from $f(\theta|y)$ using $U(0,1)$ as the proposal density. We get the following algorithm.
```{r, echo = T, eval = T}
f <- function(x){
  return((2+x)^125*(1-x)^38*x^34)
}

optimum = optimize(f,c(0,1),maximum = TRUE)

draw_samples <- function(n){
  samples <- rep(NA,n)
  iter = 0
  for (i in 1:n){
    finish = 0
    while(finish ==0){
      iter = iter + 1
      x_sample <- runif(1)
      alpha <- f(x_sample)/optimum$objective
      u = runif(1)
      if (u <=alpha){
        samples[i] <- x_sample
        finish <- 1
      }
    }
  }
  return(list(samples = samples, iterations = iter))
}
```

## 2.

Now we want to estimate the posterior mean $\theta$ by Monte-Carlo integration using $M = 10000$ samples from $f(\theta|y)$. We also draw a histogram of the samples and compare it with the analytical density, in addition to marking the estimated posterior mean.

```{r, echo 0 T, eval = T}
n = 10000
g <- function(x){
  return(f(x)*x)
}
f_density <- function(x){return(f(x)/integrate(f,0,1)$value)}
y <- draw_samples(n)
cat("Computed mean: ", sum(y$samples)/n, '\n')
cat("Theoretical mean: ", integrate(g,0,1)$value/integrate(f,0,1)$value, '\n')
df <- data.frame(theo=seq(0,1,length.out=n), value=y$samples)
ggplot(df, aes(x = value)) + geom_histogram(aes(y = ..density..), binwidth=0.001, col = 'blue', fill = 'blue') + stat_function(fun=f_density,geom = "line",size=1.6,col="red") + ggtitle("Comparison of analytical and sampled density") + xlim(0,1) + geom_segment(x = sum(y$samples)/n, xend = sum(y$samples)/n, y = 0, yend = 100, col = "green", size = 1.2, linetype = "longdash") + labs(x='x', y='density', caption=paste('Comparison of the sampled density of x_k sampled from our dirichlet sampler (red bars) and the sampler from MCMpack (blue bars)'))
```

The theoretical mean is found by numerically integrating $\text{E}(X) = \int_0^1 x f(x)dx$, and we see that it coincides well with the estimated posterior mean. From the figure we see that the density of the sample in the histogram also coincides well with the analytical density distribution, so we conclude that the sampling algorithm is correct. Also, the indicated estimated posterior mean looks reasonable, considering the density distribution.

## 3.
In theory, we would expect the rejection sampling algorithm to generate $c \geq f(x)/g(x)$ random numbers on average to obtain one sample of $f(\theta|y)$, where $g(x)$ is the proposal density. Because we want our algorithm to be as efficient as possible, we want $c$ as small as possible. The proposal density $g(x)$ is $U(0,1)$, so $g(x) = 1$. Then, we can set $c = \max{f(x)/g(x)} = \max{f(x)}$. We now compare the theoretical expected number of generated numbers with the sample average.

```{r, echo = T, eval = T}
cat("Average random numbers generated: ", y$iterations/n, '\n')
cat("Theoretical expected number: ", optimum$objective/integrate(f,0,1)$value)
```

Again, we see that the average number from the sample coincide well with the theoretical expected number.

## 4.

Based on the samples $\{\theta_i\}$ we already have, we wish to estimate the IS posterior mean when assuming a different prior. We consider the prior $\theta \sim Beta(1, 5)$, $\Pi_N(\theta) = \frac{\Gamma(1+5)}{\Gamma(1) \Gamma(5)} \theta^{0} (1-\theta)^{4}, \quad 0 \leq \theta \leq 1$. As our samples $\{\theta_i\}$ were generated from the old posterior, denoted $f_{o}$,

$$ f_{o}(\theta_i \lvert y) = (2 + \theta_i)^{y_1} ( 1 - \theta_i)^{y_2 + y_3}\theta_i^{y_4}$$

Only the prior will differ between the target distribution $f_n$ and the proposal distribution $f_o$ we can compute the weights by

$$w_i = f_n(\theta_i \lvert y) / f_o(\theta_i \lvert y) = \pi_n(\theta_i) / \pi_o(\theta_i) =  \frac{\Gamma(1+5)}{\Gamma(1) \Gamma(5)} (1-\theta_i)^{4},$$

The importance sampling posterior mean becomes 
$$ \hat{\mu}_{IS} = \frac{1}{n} \sum \theta_i w_i$$

```{r, echo = T, eval = T}
unscaled_posterior_target = function(x){
  return(f_density(x)*dbeta(x, 1, 5))
}

NC_posterior = integrate(unscaled_posterior_target, 0, 1)$value

posterior_target = function(x){
  return(unscaled_posterior_target(x) / NC_posterior) 
}

posterior_target_mean_density = function(x){
  return(x * posterior_target(x))
}

theoretical_mean = integrate(posterior_target_mean_density, 0, 1)$value

weights = posterior_target(y$samples) / f_density(y$samples)
posterior_mean = sum(y$samples*weights) / (n)
cat("IS posterior mean estimate:", posterior_mean, "\n")
cat("Analytical posterior mean:", theoretical_mean)
```

We have here computed the normalizing constant of the posterior. If this were not numerically feasible, we could use the self-normalizing importance sampling estimate of the posterior mean

$$ \tilde{\mu}_{IS} = \frac{\sum \theta_i w_i }{\sum w_i} $$

```{r}
unscaled_weights = unscaled_posterior_target(y$samples) / f(y$samples)
posterior_mean_SN = sum(y$samples*unscaled_weights) / (sum(unscaled_weights))

cat("self-normalizing IS posterior mean estimate:", posterior_mean_SN, "\n")
cat("Analytical posterior mean:", theoretical_mean)
```

We observe a slight shift in the IS estimate of the posterior mean when assuming a different prior. This is as expected, as such an estimate will fall between the Monte Carlo estimate under a uniform prior ($\sim 0.62$) and the expectation of the prior $B \sim Beta(1, 5) \Leftarrow E(B) = 1 / (1+5) = 0.167$. 

Under the uniform prior, we calculated the Monte Carlo estimate of the posterior mean to be $\sim 0.62$. We investigate what proportion of its mass the $Beta(1, 5)$ prior has on the left side of $0.62$

```{r, eval=T, echo=T}
dbeta2 = function(x){
  return(dbeta(x, 1, 5))
}
est = 0.62
cat(paste('Mass left of x =',est,':', integrate(dbeta2, 0, est)$value))
```

We observe that $Beta(1,5)$ has the majority of its mass to the left of our Monte Carlo posterior mean estimate. Assuming the $Beta(1,5)$ prior is reasonable if we have a priori knowledge that $\theta$ might be much smaller than $0.62$.   

Simply by assuming a new prior, we have induced a non-negligible change in the estimate of the posterior mean. In this case, it is probable that the $Beta(1,5)$ prior brings us away from the true distribution of $\theta$. This is an example of the dangers associated with the Bayesian approach; while assuming a reasonable prior may give increased accuracy in the results, wrong assumptions may induce a bias. Some might call this another facet of the bias-variance trade-off. 



