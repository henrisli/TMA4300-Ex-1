--- 
title: 'TMA4300 Computer Intensive Statistical Methods Exercise 1, Spring 2019'
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Henrik Syversveen Lie, Mikal Stapnes (SKRIVE STUDENTNUMMER I STEDET??)'
---


```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```

# Problem A: Stochastic simulation by the probability integral transform and bivariate techniques

## 1.
We make a function that draws $n$ random numbers from an exponential distribution with parameter $\lambda$. If $u$ is uniformly distributed between 0 and 1, $u\sim U(0,1)$, then $x=-\log(u)/\lambda$ will be exponentially distributed with parameter $\lambda$, or $x\sim \text{Exp}(\lambda)$.
```{r, echo = T, eval = T}
library(ggplot2)
exponential<- function(lambda, n=1){
  u = runif(n)
  x = -1/lambda*log(u)
  return(x)
}
```
Then we want to check if this function is correct. To do this, we check a few things. Firstly we compare the mean and variance of a sample drawn from our function with the theoretical mean and variance. The theoretical mean and variance of a exponential distribution are $\mu = 1/\lambda$ and $\sigma^2 = 1/\lambda^2$. We draw a sample with $\lambda = 4.300$ and compare:
```{r, echo = T, eval = T}
n = 10000
lambda = 4.300
x = exponential(lambda,n)
cat('Theoretical mean: ', 1/lambda,' Computed mean: ', mean(x),'\n')
cat('Theoretical variance: ', 1/lambda^2, ' Computed variance: ', var(x))
```
We see that they are almost the same. Finally we make two plots. The first is a histogram comparison of our function and the R function `rexp()`, which draws random samples from an exponential distribution. The second plot is a histogram of our random sample compared with the theoretical exponential distribution.
```{r, echo = T, eval = T}
y = rexp(n, lambda)
rfunc = data.frame(value = y)
ourfunc = data.frame(value = x)
rfunc$type = 'R function'
ourfunc$type = 'Our function'
df = rbind(rfunc,ourfunc)
ggplot(df, aes(value, fill = type)) + 
   geom_histogram(alpha = 0.5, aes(y = ..density..), binwidth = 0.05, position = 'identity') + ggtitle("Our function vs. R function densities")
df2 <- data.frame(theo=seq(0,max(x),length.out=n), value=x)
h <- ggplot(df2, aes(x = value))
h <- h + geom_histogram(aes(y = ..density..), binwidth=0.05, fill = 'blue', color = 'blue') +stat_function(fun=dexp,geom = "line",size=1.6,col="red",args = (mean=lambda))
h <- h + ggtitle("Our function vs. theoretical density")
h
```

## 2. a)
We first note that the normalizing constant will be $c=\frac{e\alpha}{e+\alpha}$, which can be found by solving $$\int_0^{\infty} g(x)dx = 1.$$

We find the cumulative distribution by noting that $G_X(x) = P(X\leq x)$, which gives 
$$G_X(x) = \int_0^x g(\xi)d\xi = \begin{cases} \int_0^x c\xi^{\alpha-1}d\xi = \frac{cx^{\alpha}}{\alpha}, \quad &0\leq x\leq 1,\\ \frac{c}{\alpha}+\int_1^x c\exp(-\xi)d\xi = \frac{c}{\alpha}+\frac{c}{e}-c\exp(-x), \quad &1\leq x,\\0, &\text{otherwise}.\end{cases}$$
Because $c=\frac{e\alpha}{e+\alpha}$, we have that $c/\alpha + c/e=1$, giving

$$G_X(x) = \begin{cases} \frac{cx^{\alpha}}{\alpha}, \quad &0\leq x\leq 1,\\ 1-c\exp(-x), \quad &1\leq x,\\0, &\text{otherwise}.\end{cases}$$
Then we take the inverse of this function to obtain
$$G_X^{-1}(u) = \begin{cases} (\frac{\alpha}{c}u)^{1/\alpha}, \quad &0 \leq u<\frac{e}{\alpha+e}, \\
\ln(\frac{c}{1-u}), \quad &\frac{e}{\alpha+e} \leq u \leq 1.
\end{cases}$$
This can be used to sample from $g(x)$. To confirm this, we sample from $u\sim U(0,1)$ and compute the inverse $X = G^{-1}_X(u)$. Then our claim is that $X \sim g(x)$. We start by computing the analytical mean of our distribution and compare this with the mean of the drawn sample. We find the mean by
$$\text{E}(X) = \int_0^{\infty} xg(x)dx = \frac{c}{\alpha+1} + \frac{2c}{e}.$$
We compare results in R.

```{r, echo = T, eval = T}
f_A2 = function(x, alpha){
  c = alpha*exp(1)/(alpha+exp(1))
  result = vector(length = length(x))
  result[x<1.] = sapply(x[x<1.], function(x) c*x^(alpha - 1))
  result[x>=1.] = sapply(x[x>=1.], function(x) c*exp(-x))
  return(result)
}

F_inv = function(n, alpha){
  c = alpha*exp(1)/(alpha+exp(1))
  u = runif(n)
  result = vector(length = n)
  result[u < c/alpha] = (alpha/c * u[u < c/alpha])^(1/alpha)
  result[u >= c/alpha] = -log(1/alpha + exp(-1) - u[u >= c/alpha]/c)
  return(result)
}

alpha = 2.5
n = 10000
xsamples = F_inv(n, alpha)
c = alpha*exp(1)/(alpha+exp(1))
mean = c/(alpha+1) + 2*c/exp(1)
cat('Theoretical mean: ', mean, ' Sampled mean: ', mean(xsamples))
```
Then we compute the theoretical variance by $\text{Var}(X) = \text{E}(X^2)-\text{E}(X)^2$ and compare it with the sampled variance. The theoretical variance will be
$$\text{Var}(X) = \text{E}(X^2) - \text{E}(X)^2 = \frac{c}{\alpha+2}+\frac{5c}{e} - \big(\frac{c}{\alpha+1} + \frac{2c}{e}\big)^2.$$

We compare results in R
```{r, echo = T, eval = T}
variance = c/(alpha+2)+5*c/exp(1) - mean^2
cat('Theoretical variance: ', variance, ' Sampled variance: ', var(xsamples))
```


Finally we plot a histogram of the sample and compare it to the density function $g(x)$.

```{r, echo = T, eval = T}
df2 <- data.frame(theo=seq(0,max(xsamples),length.out=n), value=xsamples)
h <- ggplot(df2, aes(x = value))
h <- h + geom_histogram(aes(y = ..density..), binwidth=0.1, col = 'blue', fill = 'blue') +stat_function(fun=f_A2,geom = "line",size=1.6,col="red",args = (mean=alpha))
h <- h + ggtitle("Comparison of analytical and sampled density")
h
```

We see that the analytical expression overlaps perfectly with the histogram over the samples, confirming the our algorithm is correct.

## 3.
To simulate from the standard normal distribution, we implement a function that performs the Box-Müller transformation. Given $x_1 \sim U(0,1)$ and $x_2 \sim \text{Exp}(1/2)$, we have that $y_1 = \sqrt{x_2}\cos{(2\pi x_1)}$ and $y_2 = \sqrt{x_2}\sin{(2\pi x_1)}$ will be i.i.d. standard normal. To sample from the exponential distribution, we use the `exponential` function implemented earlier. The following function implements the Box-Müller transformation to generate $n$ independent samples from the standard normal distribution. To reduce runtime of the algorithm, we assume $n$ to be an even number, so that we can generate only $n/2$ samples from the uniform and exponential distributions and use both $y_1$ and $y_2$.
```{r, echo = T, eval = T}
normal <- function(n){
  x_1 <- runif(n/2)
  x_2 <- exponential(1/2, n/2)
  y_1 <- sqrt(x_2)*cos(2*pi*x_1)
  y_2 <- sqrt(x_2)*sin(2*pi*x_1)
  y <- c(y_1,y_2)
  return(y)
}
```

Then we want to verify the algorithm, so we compute the mean and variance of the sample.
```{r, echo = T, eval = T}
n = 100000
x <- normal(n)
cat('Theoretical mean: ', 0,' Computed mean: ', mean(x),'\n')
cat('Theoretical variance: ', 1, ' Computed variance: ', var(x))
```
We see that the computed mean and variance coincide with the theoretical mean and variance of 0 and 1. 

Finally we make a histogram of our sample and compare it with the analytical density of the standard normal distribution. In addition, we provide a Q-Q plot to check normality of the data.

```{r, echo = T, eval = T}
df2 <- data.frame(theo=seq(0,max(x),length.out=n), value=x)
ggplot(df2, aes(x = value)) + geom_histogram(aes(y = ..density..), binwidth=0.1, col = 'blue', fill = 'blue') + stat_function(fun=dnorm,geom = "line",size=1.6,col="red",args = list(mean=0, sd = 1)) + ggtitle("Comparison of analytical and sampled density")

x <- normal(1000)
qqnorm(x, pch = 1, frame = FALSE)
qqline(x, col = "steelblue", lwd = 2)
```
Also here, the histogram coincides well with the standard normal density function, so we conclude that our algorithm is correct. Lastly, the sampled points in the Q-Q plot coincide with the theoretical line.

## 4.
We now implement a function that generates samples from a $d$-variate normal distribution with mean vector $\mu$ and covariance matrix $\Sigma$. We use the function `normal`, which we created in task A.3. to generate $d$ i.i.d. standard normal samples. Then we use the transformation $y = \mu + Ax \sim N(\mu, A A^T)$, with $AA^T=\Sigma$ and $x$ being a vector with the independent standard normal samples. We use the Cholesky decomposition for A.
```{r, echo = T, eval = T}
normal_d <- function(d, mu, sigma){
  x <- normal(d)
  y <- mu + t(chol(sigma))%*%x
  return(y)
}
```

Then, we draw samples from a $d$-variate distribution to check our results. We use
$$\mu = \begin{pmatrix} 1 \\ 2 \end{pmatrix}, \quad \Sigma = \begin{pmatrix} 2.5 & 3.0 \\ 3.0 & 10.0\end{pmatrix}.$$
We check if the generated sample has the correct mean vector and covariance matrix to see if the generated sample truly is $d$-variate normal.
```{r, echo = T, eval = T}
d = 2
n = 100000
mu = c(1,2)
sigma = matrix(c(2.5,3,3,10),ncol=2,nrow=2)
sample = matrix(NA, ncol = d, nrow = n)
for (i in 1:n){
  sample[i,] <- normal_d(d,mu,sigma)
}
average = apply(sample, 2,mean)
cat('Mean vector: \n')
average
covmat = cov(sample)
cat('Covariance matrix: \n')
covmat
```
Both the mean vector and the covariance matrix coincide well with the theoretical mean and covariance.

Finally we implement an Anderson-Darling normality test to check the normality of the data. We check that each of the two elements are normal.
```{r, echo = T, eval = T}
library(nortest)
ad.test(sample[,1])
ad.test(sample[,2])
```
Both p-values are above any reasonable significance level, which does not reject the hypothesis that the data is normal distributed, and we conclude that our algorithm is correct.

# Problem B: The gamma distribution

## 1.

We can sample from the gamma distribution with $\alpha \in (0, 1)$ and $\beta = 1$,

$$ f(x) = \begin{cases}
\frac{1}{\Gamma(\alpha)}x^{\alpha-1}e^{-x}, \quad & 0<x,\\
0, \quad &\text{otherwise}. \\
\end{cases} $$

using the distribution found in A.2 as a proposal distribution. We can do this because the A.2 distribution is nonzero at all points where $f(x)$ is nonzero. 

```{r, echo = T, eval = T}
library(pracma)

#x = linspace(0, 3, 100)
#alpha = 0.5
#y_gamma = dgamma(x, shape=alpha, rate=1)
#y_prop = 1/(gamma(alpha)) * (1/alpha + exp(-1))*f_A2(x, alpha)

#df10 = data.frame(x, y_gamma, y_prop)
#h <- ggplot(data.frame(x, y_gamma, y_prop))
#h = h + geom_line(aes(x = x, y = y_gamma, colour="Gamma"), size=1.)
#h = h + geom_line(aes(x = x, y = y_prop, color="Proposal"), size=1.)
#h = h + ggtitle(paste("Gamma(",toString(alpha),", 1) vs. proposal"))
#h

```

We note that the acceptance probabiltiy, P_{accept}, 

$$P_{accept} = \int_0^{\infty} P_{unif}(U \leq \frac{f(x)}{c g(x)} ) P_{prop}(X = x) \\
 = \int_0^{\infty} \frac{f(x)}{c g(x)} g(x) = c^{-1},$$
 
To maximize the acceptance probability, we can minimize $c$ by choosing

$$ c = \underset{x}{\sup} \frac{f(x)}{g(x)} = \underset{x}{\sup} \begin{cases}
\frac{e^{-x}(\frac{1}{\alpha} + \frac{1}{e})}{\Gamma(\alpha)}, \quad 0 \leq x \le 1 \\
\frac{x^{\alpha - 1} (\frac{1}{\alpha} + \frac{1}{e})}{\Gamma(\alpha)}, \quad 1 \leq x
\end{cases}$$ 

Which means $c = \frac{(\frac{1}{\alpha} + \frac{1}{e})}{\Gamma(\alpha)}$

To generate $n$ samples from $f(x)$ we sample from $X \sim g(x)$ and $U \sim u[0, 1]$, and accept the sample $x$ if $u \leq \frac{f(x)}{c g(x)}$. We repeat the process until $n$ samples are accepted. 

We compare the sample mean and variance with the theoretical mean and variance and compare the sampled density with the theoretical density. 

```{r}
library(pracma)

small_gamma = function(alpha, n=1){
  cat("Small gamma \n")
  result = vector()
  iter = 0
  sampled = 0
  while(sampled < n){

    uniform2 = runif(n)
    x = F_inv(n, alpha)
    cat(x)
    cat(alpha)
    c = 1/(gamma(alpha)) * (1/alpha + exp(-1))
  
    cat(c)
    probs = dgamma(x, alpha, rate=1) / (c * f_A2(x, alpha))
    
    result = append(result, x[uniform2 <= probs])
    
    sampled = sampled + sum(uniform2 <= probs)
    iter = iter + n
  }
  
  iter = iter - length(result) + n
  
  return(list(samples = result[seq(1:n)], iter = iter))
}

large_gamma <- function(alpha, n=1){
  #cat("Large gamma \n")
  #a = sqrt((alpha - 1)^(alpha-1)*exp(1-alpha))
  loga = 1/2 * (alpha - 1) * (log(alpha - 1) - 1)
  logb = 1/2 * (alpha + 1) * (log(alpha + 1) - 1)
  #cat("Sampling for alpha = ", alpha, '\n')
  #cat("loga = ", loga, "\n")
  #cat("logb = ", logb, "\n")

  result = vector(mode="numeric")
  
  i = 0
  iter = 0
  while (i < n){
    iter = iter + 1 
    log_x_1 = loga + log(runif(1))
    log_x_2 = logb + log(runif(1))
    #cat(log_x_1, log_x_2)
    #y = x_2 / x_1
    if (2*log_x_1 <= (alpha - 1)*(log_x_2 - log_x_1) - exp(log_x_2 - log_x_1)){
      result = append(result, exp(log_x_2 - log_x_1))
      i = i + 1
    }
    else{}
  }
  list(samples = result, iter = iter)
}

gamma2 = function(alpha, n  = 1){
  if (alpha <= 1){
    return(small_gamma(alpha, n))
  }
  else if (alpha > 1){
    return(large_gamma(alpha, n))
  }
  else{
    cat("Weird gamma \n")
  }
}
```
```{r, echo = T, eval = T}
alpha = 0.5
G1 <- small_gamma(alpha, 10000)

G1_filt = G1
df12 = data.frame(G1_filt)

cat('Theoretical mean: ', alpha,' Computed mean: ', mean(G1$samples),'\n')
cat('Theoretical variance: ', alpha, ' Computed variance: ', var(G1$samples))

c = 1/(gamma(alpha)) * (1/alpha + exp(-1))

x_val = linspace(0, 5, 100)

df11 <- data.frame(x = x_val, y=dgamma(x_val, alpha, 1))

h <- ggplot()
h <- h + geom_histogram(data=df12, aes(x=samples, y=..density.., color="Sampled"), binwidth=0.1)
h <- h + geom_line(data=df11, aes(x=x, y=y, color="Theoretical"), size=1.6)
h <- h + ggtitle("Comparison of analytical and sampled density")
h
```

From the moments and the figure we see that we get the correct distribution when using rejection sampling. 

## 2.

As we remove the limitation on $\alpha$, we no longer get a closed form expression of the optimal $c$ and this becomes an optimization problem. Instead, we may use the ratio of uniforms method to sample from the Gamma with parameters $\alpha > 1$, $\beta = 1$. We define the area

$$C_f = \bigg \{(x_1, x_2): 0 \leq x_1 \leq \sqrt{f^*(\frac{x_2}{x_1})} \bigg \}, \quad f^*(x) = \begin{cases}
x^{\alpha-1}e^{-x}, \quad 0 < x, \\
0, \quad \text{otherwise}.
\end{cases}$$ 

And note that $y = x_2 / x_1$ will be Gamma-distributed if sampled uniformly inside $C_f$. By finding 

$$ a = \sqrt{(\underset{x}{\sup} f^* (x)}, \quad b_{+} = \sqrt{ \underset{x\geq 0}{\sup} x^2 f^* (x)} , \quad b_{\_} = - \sqrt{\underset{x \leq 0}{\sup} x^2 f^* (x)}$$


$$ (f^*(x))' = 0 \Leftrightarrow x^{\alpha-2}e^{-x}(\alpha - 1  - x) = 0 \Rightarrow x = \alpha - 1 \\
a = \sqrt{(\alpha - 1)^{\alpha - 1}e^{1-\alpha}}$$

$$(x^2 f^*(x))' = 0 \Leftrightarrow x^{\alpha}e^{-x}(\alpha + 1 - x) = 0 \\
b_{+} = \sqrt{(\alpha + 1)^{\alpha + 1}e^{-\alpha - 1}}$$

$$b_{-} = 0$$

We can sample uniformly from $[0, a]\times[b_{-}, b_{+}]$ and select the subset of samples that are also in $C_f$. Then $y = \frac{x_2}{x_1}$ will be Gamma($\alpha ,1$) distributed. 

```{r}
alpha = 1.5

G2 = gamma2(alpha, 10000)

cat('Theoretical mean: ', alpha,' Computed mean: ', mean(G2$samples),'\n')
cat('Theoretical variance: ', alpha, ' Computed variance: ', var(G2$samples))

df16 = data.frame(data=G2)

h <- ggplot(df16, aes(x=data.samples))
h <- h + geom_histogram(data=df16, aes(x = data.samples, y=..density.., color="Sampled Function"), binwidth=0.1) 
h <- h + stat_function(fun=dgamma,geom = "line",size=1.6,col="red",args = list(shape=alpha, rate = 1))
h

```

As 

```{r}
iterations = c()

for (alpha in seq(2, 2000, 10)){
  G = gamma2(alpha, 1000)
  iterations = append(iterations, G$iter)
}

df15 = data.frame(alpha = seq(5, 2000, 10), iterations = iterations)

h <- ggplot(df15, aes(x = alpha, y= iterations, color="Iterations"))
h <- h + geom_line()
h
```



# Problem C: The Dirichlet distribution: simulation using known relations

## 1.
We assume $z_k\sim \Gamma(\alpha_k,1)$ for $k=1,\ldots,K$ independently, and define $x_k = z_k/(z_1+\dots+z_K)$ for $k=1,\ldots,K$. Then we want to find the distribution of $x= (x_1,\ldots,x_K)$. We use a transformation of the joint distribution of $z = (z_1,\ldots,z_K)$ to $(x_1,\ldots,x_{K-1},v), v = z_1+\dots+z_K$. The inverse transformation will be $z_k = vx_k, k = 1,\ldots,K$, which has a Jacobian
$$\lvert J \rvert = \begin{vmatrix}\frac{\partial z_1}{\partial x_1}  & \dots & \frac{\partial z_1}{\partial x_{K-1}} & \frac{\partial z_1}{\partial v}\\ \vdots &  \ddots & \vdots & \vdots \\ \frac{\partial z_K}{\partial x_1} & \dots & \frac{\partial z_K}{\partial x_{K-1}}&\frac{\partial z_K}{\partial v} \end{vmatrix} = \begin{vmatrix}v&0  & \dots & 0 & x_1\\ 0&v&\dots & 0 & x_2\\\vdots &\vdots&  \ddots & \vdots & \vdots \\0 & 0& \dots&v&x_{K-1}\\ -v &-v& \dots & -v&1-\sum_{k=1}^{K-1}x_k  \end{vmatrix} = v^{K-1}.$$
Then, we note that the joint distribution of $z_k, k = 1,\ldots,K$ will be
$$F_{z_1,\ldots,z_K}(z_1,\ldots,z_K) = \prod_{k=1}^K \frac{1}{\Gamma(\alpha_k)}z_k^{\alpha_k-1}\exp(-z_k)=\prod_{k=1}^K \bigg(\frac{1}{\Gamma(\alpha_k)}z_k^{\alpha_k-1}\bigg)\exp(-\sum_{k=1}^K z_k).$$
Now, we do the transformation with $z_k = vx_k, k = 1,\ldots,K-1$ and $v = z_1+\dots+z_K$ to get
\begin{align*}F_{x_1,\ldots,x_{K-1},v}(x_1,\ldots,x_{K-1},v) &= \prod_{k=1}^{K-1} \bigg(\frac{1}{\Gamma(\alpha_k)}(vx_k)^{\alpha_k-1}\bigg)\cdot\frac{1}{\Gamma(\alpha_K)}(v\big(1-\sum_{k=1}^{K-1}x_k\big))^{\alpha_K-1}\exp(-v)v^{K-1}\\
&= \prod_{k=1}^{K}\bigg(\frac{1}{\Gamma(\alpha_k)}\bigg)v^{\big(\sum_{k=1}^K\alpha_k\big)-1}\exp(-v)\bigg(\prod_{k=1}^{K-1}x_k^{\alpha_k-1}\bigg)\bigg(1-\sum_{k=1}^{K-1}x_k\bigg)^{\alpha_K-1}.
\end{align*}

Finally, we want to find the marginal distribution of $F_{x_1,\ldots,x_{K-1}}(x_1,\ldots,x_{K-1})$, so we integrate out $v$. The variable $v=z_1+\dots+z_k$ is a sum of Gamma distributed variables, so it can vary between 0 and $\infty$. We note that the Gamma function is defined by
$$\Gamma(z) = \int_0^\infty \exp(-x)x^{z-1}dx,$$
which gives $$\int_0^\infty \exp(-v)v^{\big(\sum_{k=1}^K\alpha_k\big)-1}dv = \Gamma(\sum_{k=1}^K\alpha_k),$$
and in total we have
$$ F_{x_1,\ldots,x_{K-1}}(x_1,\ldots,x_{K-1})=\bigg(\frac{\Gamma(\sum_{k=1}^K\alpha_k)}{\prod_{k=1}^{K}\Gamma(\alpha_k)}\bigg)\bigg(\prod_{k=1}^{K-1}x_k^{\alpha_k-1}\bigg)\bigg(1-\sum_{k=1}^{K-1}x_k\bigg)^{\alpha_K-1}.$$
We also know that $\sum_{k=1}^Kx_k=1$, $x_1,\ldots,x_{K-1}>0$ and $\sum_{k=1}^{K-1}x_k<1$, which means that $x=(x_1,\ldots,x_K)$ will be Dirichlet distributed.

## 2.
Now we want to implement a function that draws one realization from the Dirichlet distribution with parameter vector $(\alpha_1,\ldots,\alpha_K)$. We do so in the following code.
```{r, echo = T, eval = T}
library(MCMCpack)
different_alphas <- function(alpha){
  
}
dirichlet <- function(alpha){
  z <- rep(NA, length(alpha))
  for (i in 1:length(alpha)){
    z[i] = gamma2(alpha[i])
  }
  #z<- sapply(alpha,gamma2)
  x <- z/sum(z)
  return(x)
}
```
We want to test if the function draws from the correct distribution. To do so, we first check the mean and variance of a generated sample. From theory, it is known that if $(x_1,\ldots,x_K)\sim \text{Dirichlet}(\alpha_1,\ldots,\alpha_K)$, then
$$\text{E}(X_k) = \frac{\alpha_k}{\alpha_0}, \quad \text{Var}(X_i) = \frac{\alpha_i(\alpha_0-\alpha_i)}{\alpha_0^2(\alpha_0+1)}, \quad \alpha_0 = \sum_{i=1}^K \alpha_i.$$
To determine $(\alpha_1,\ldots,\alpha_k)$, we simulate them from an exponential distribution with $\lambda = 1$ to ensure that all $\alpha$ are positive. We then compare sample mean and variance with theoretical mean and variance for a sample with $k=5$.
```{r, echo = T, eval = T}
k = 5
n = 100000
x <- matrix(NA, ncol = k, nrow = n)
alpha <- exponential(1,k)
for (i in 1:n){
  x[i,] <- dirichlet(alpha)
}
cat('Theoretical mean: ', alpha/sum(alpha),'\n','Computed mean: ', apply(x,2,mean),'\n')
cat('Theoretical variance: ', alpha*(sum(alpha)-alpha)/(sum(alpha)^2*(sum(alpha)+1)),'\n', 'Computed variance: ', apply(x,2,var))
```
We see that the mean and variance coincide well for all $x_k$. To further confirm that our implementation is correct, we compare a sample drawn from our function with the already implemented R function `rdirichlet` from the `MCMCpack` library. For simplicity, we let all $\alpha_k=1$, so each $x_k$ has the same marginal distribution. We then make a histogram of all $x_k$ from the two samples.

```{r, echo = T, eval = T}
n <- 50000
alpha <- rep(1,k)
x <- matrix(NA, ncol = k, nrow = n)
for (i in 1:n){
  x[i,] <- dirichlet(alpha)
}
y <- as.vector(rdirichlet(n,alpha))
rfunc = data.frame(value = y)
ourfunc = data.frame(value = as.vector(x))
rfunc$type = 'R function'
ourfunc$type = 'Our function'
df = rbind(rfunc,ourfunc)
ggplot(df, aes(value, fill = type)) + 
   geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity') + ggtitle("Our function vs. R function densities")
```
Also here, the histograms coincide well. As a final test, we plot a histogram of our generated sample and compare it with the theoretical density function. Because the Dirichlet distribution is a multivariate distribution, we only investigate the marginal distribution of $x_1$, and it can be shown that the marginal distribution of any $x_k$ is the Beta distribution with parameters $\alpha_k$ and $\sum_{i\neq k}\alpha_i$. This means that we can plot the density of $x_1$ from our generated sample and compare it with a Beta distribution density. Again, for simplicity we assume all $\alpha_k=1$.

```{r, echo = T, eval = T}
df2 <- data.frame(theo=seq(0,max(x[,1]),length.out=n), value=x[,1])
ggplot(df2, aes(x = value)) + geom_histogram(aes(y = ..density..), 
      binwidth = 0.005,fill = 'blue', color = 'blue') + stat_function(fun=dbeta,geom = "line",size=1.6,col="red",args = list(shape1=1,shape2=k-1)) + ggtitle("Our function vs. theoretical density")
```

The histogram fits the theoretical density well, and we conclude that we have implemented the function to generate Dirichlet samples correctly.

# Problem D: Rejection sampling and importance sampling