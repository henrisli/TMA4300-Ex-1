--- 
title: 'TMA4300 Computer Intensive Statistical Methods Exercise 1, Spring 2019'
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Henrik Syversveen Lie, Mikal Stapnes (SKRIVE STUDENTNUMMER I STEDET??)'
---


```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```

# Problem A: Stochastic simulation by the probability integral transform and bivariate techniques

## 1.
We make a function that draws $n$ random numbers from an exponential distribution with parameter $\lambda$. If $u$ is uniformly distributed between 0 and 1, $u\sim U(0,1)$, then $x=-\log(u)/\lambda$ will be exponentially distributed with parameter $\lambda$, or $x\sim \text{Exp}(\lambda)$.
```{r, echo = T, eval = T}
library(ggplot2)
exponential<- function(lambda, n){
  u = runif(n)
  x = -1/lambda*log(u)
  return(x)
}
```
Then we want to check if this function is correct. To do this, we check a few things. Firstly we compare the mean and variance of a sample drawn from our function with the theoretical mean and variance. The theoretical mean and variance of a exponential distribution are $\mu = 1/\lambda$ and $\sigma^2 = 1/\lambda^2$. We draw a sample with $\lambda = 4.300$ and compare:
```{r, echo = T, eval = T}
n = 10000
lambda = 4.300
y = rexp(n, lambda)
x = exponential(lambda,n)
cat('Theoretical mean: ', 1/lambda,' Computed mean: ', mean(x),'\n')
cat('Theoretical variance: ', 1/lambda^2, ' Computed variance: ', var(x))
```
We see that they are almost the same. Finally we make two plots. The first is a histogram comparison of our function and the R function `rexp()`, which draws random samples from an exponential distribution. The second plot is a histogram of our random sample compared with the theoretical exponential distribution.
```{r, echo = T, eval = T}
rfunc = data.frame(value = y)
ourfunc = data.frame(value = x)
rfunc$type = 'R function'
ourfunc$type = 'Our function'
df = rbind(rfunc,ourfunc)
ggplot(df, aes(value, fill = type)) + 
   geom_histogram(alpha = 0.5, aes(y = ..density..), binwidth = 0.05, position = 'identity') + ggtitle("Our function vs. R function densities")
df2 <- data.frame(theo=seq(0,max(x),length.out=n), value=x)
h <- ggplot(df2, aes(x = value))
h <- h + geom_histogram(aes(y = ..density..), binwidth=0.05, fill = 'blue', color = 'blue') +stat_function(fun=dexp,geom = "line",size=1.6,col="red",args = (mean=lambda))
h <- h + ggtitle("Our function vs. theoretical density")
h
```

## 2. a)
We first note that the normalizing constant will be $c=\frac{e\alpha}{e+\alpha}$, which can be found by solving $$\int_0^{\infty} g(x)dx = 1.$$

We find the cumulative distribution by noting that $G_X(x) = P(X\leq x)$, which gives 
$$G_X(x) = \int_0^x g(\xi)d\xi = \begin{cases} \int_0^x c\xi^{\alpha-1}d\xi = \frac{cx^{\alpha}}{\alpha}, \quad &0\leq x\leq 1,\\ \frac{c}{\alpha}+\int_1^x c\exp(-\xi)d\xi = \frac{c}{\alpha}+\frac{c}{e}-c\exp(-x), \quad &1\leq x,\\0, &\text{otherwise}.\end{cases}$$
Because $c=\frac{e\alpha}{e+\alpha}$, we have that $c/\alpha + c/e=1$, giving

$$G_X(x) = \begin{cases} \frac{cx^{\alpha}}{\alpha}, \quad &0\leq x\leq 1,\\ 1-c\exp(-x), \quad &1\leq x,\\0, &\text{otherwise}.\end{cases}$$
Then we take the inverse of this function to obtain
$$G_X^{-1}(u) = \begin{cases} (\frac{\alpha}{c}u)^{1/\alpha}, \quad &0 \leq u<\frac{e}{\alpha+e}, \\
\ln(\frac{c}{1-u}), \quad &\frac{e}{\alpha+e} \leq u \leq 1.
\end{cases}$$
This can be used to sample from $g(x)$. To confirm this, we sample from $u\sim U(0,1)$ and compute the inverse $X = G^{-1}_X(u)$. Then our claim is that $X \sim g(x)$. We start by computing the analytical mean of our distribution and compare this with the mean of the drawn sample. We find the mean by
$$\text{E}(X) = \int_0^{\infty} xg(x)dx = \frac{c}{\alpha+1} + \frac{2c}{e}.$$
We compare results in R.

```{r, echo = T, eval = T}
f_A2 = function(x, alpha){
  c = alpha*exp(1)/(alpha+exp(1))
  result = vector(length = length(x))
  result[x<1.] = sapply(x[x<1.], function(x) c*x^(alpha - 1))
  result[x>=1.] = sapply(x[x>=1.], function(x) c*exp(-x))
  return(result)
}

F_inv = function(n, alpha){
  c = alpha*exp(1)/(alpha+exp(1))
  u = runif(n)
  result = vector(length = n)
  result[u < c/alpha] = (alpha/c * u[u < c/alpha])^(1/alpha)
  result[u >= c/alpha] = -log(1/alpha + exp(-1) - u[u >= c/alpha]/c)
  return(result)
}

alpha = 2.5
n = 10000
xsamples = F_inv(n, alpha)
c = alpha*exp(1)/(alpha+exp(1))
mean = c/(alpha+1) + 2*c/exp(1)
cat('Theoretical mean: ', mean, ' Sampled mean: ', mean(xsamples))
```
Then we compute the theoretical variance by $\text{Var}(X) = \text{E}(X^2)-\text{E}(X)^2$ and compare it with the sampled variance. The theoretical variance will be
$$\text{Var}(X) = \text{E}(X^2) - \text{E}(X)^2 = \frac{c}{\alpha+2}+\frac{5c}{e} - \big(\frac{c}{\alpha+1} + \frac{2c}{e}\big)^2.$$

We compare results in R
```{r, echo = T, eval = T}
variance = c/(alpha+2)+5*c/exp(1) - mean^2
cat('Theoretical variance: ', variance, ' Sampled variance: ', var(xsamples))
```


Finally we plot a histogram of the sample and compare it to the density function $g(x)$.

```{r, echo = T, eval = T}
df2 <- data.frame(theo=seq(0,max(xsamples),length.out=n), value=xsamples)
h <- ggplot(df2, aes(x = value))
h <- h + geom_histogram(aes(y = ..density..), binwidth=0.1, col = 'blue', fill = 'blue') +stat_function(fun=f_A2,geom = "line",size=1.6,col="red",args = (mean=alpha))
h <- h + ggtitle("Comparison of analytical and sampled density")
h
```

We see that the analytical expression overlaps perfectly with the histogram over the samples, confirming the our algorithm is correct.

## 3.
To simulate from the standard normal distribution, we implement a function that performs the Box-Müller transformation. Given $x_1 \sim U(0,1)$ and $x_2 \sim \text{Exp}(1/2)$, we have that $y_1 = \sqrt{x_2}\cos{(2\pi x_1)}$ and $y_2 = \sqrt{x_2}\sin{(2\pi x_1)}$ will be i.i.d. standard normal. To sample from the exponential distribution, we use the `exponential` function implemented earlier. The following function implements the Box-Müller transformation to generate $n$ independent samples from the standard normal distribution. To reduce runtime of the algorithm, we assume $n$ to be an even number, so that we can generate only $n/2$ samples from the uniform and exponential distributions and use both $y_1$ and $y_2$.
```{r, echo = T, eval = T}
normal <- function(n){
  x_1 <- runif(n/2)
  x_2 <- exponential(1/2, n/2)
  y_1 <- sqrt(x_2)*cos(2*pi*x_1)
  y_2 <- sqrt(x_2)*sin(2*pi*x_1)
  y <- c(y_1,y_2)
  return(y)
}
```

Then we want to verify the algorithm, so we compute the mean and variance of the sample.
```{r, echo = T, eval = T}
n = 100000
x <- normal(n)
cat('Theoretical mean: ', 0,' Computed mean: ', mean(x),'\n')
cat('Theoretical variance: ', 1, ' Computed variance: ', var(x))
```
We see that the computed mean and variance coincide with the theoretical mean and variance of 0 and 1. 

Finally we make a histogram of our sample and compare it with the analytical density of the standard normal distribution. In addition, we provide a Q-Q plot to check normality of the data.

```{r, echo = T, eval = T}
df2 <- data.frame(theo=seq(0,max(x),length.out=n), value=x)
h <- ggplot(df2, aes(x = value))
h <- h + geom_histogram(aes(y = ..density..), binwidth=0.1, col = 'blue', fill = 'blue') +stat_function(fun=dnorm,geom = "line",size=1.6,col="red",args = list(mean=0, sd = 1))
h <- h + ggtitle("Comparison of analytical and sampled density")
h
x <- normal(1000)
qqnorm(x, pch = 1, frame = FALSE)
qqline(x, col = "steelblue", lwd = 2)
```
Also here, the histogram coincides well with the standard normal density function, so we conclude that our algorithm is correct. Lastly, the sampled points in the Q-Q plot coincide with the theoretical line.

## 4.
We now implement a function that generates samples from a $d$-variate normal distribution with mean vector $\mu$ and covariance matrix $\Sigma$. We use the function `normal`, which we created in task A.3. to generate $d$ i.i.d. standard normal samples. Then we use the transformation $y = \mu + Ax \sim N(\mu, A A^T)$, with $AA^T=\Sigma$ and $x$ being a vector with the independent standard normal samples. We use the Cholesky decomposition for A.
```{r, echo = T, eval = T}
normal_d <- function(d, mu, sigma){
  x <- normal(d)
  y <- mu + t(chol(sigma))%*%x
  return(y)
}
```

Then, we draw samples from a $d$-variate distribution to check our results. We use
$$\mu = \begin{pmatrix} 1 \\ 2 \end{pmatrix}, \quad \Sigma = \begin{pmatrix} 2.5 & 3.0 \\ 3.0 & 10.0\end{pmatrix}.$$
We check if the generated sample has the correct mean vector and covariance matrix to see if the generated sample truly is $d$-variate normal.
```{r, echo = T, eval = T}
d = 2
n = 100000
mu = c(1,2)
sigma = matrix(c(2.5,3,3,10),ncol=2,nrow=2)
sample = matrix(rep(1,d*n), ncol = d, nrow = n)
for (i in 1:n){
  sample[i,] <- normal_d(d,mu,sigma)
}
average = apply(sample, 2,mean)
average
covmat = cov(sample)
covmat
```
Both the mean vector and the covariance matrix coincide well with the theoretical mean and covariance.

Finally we implement an Anderson-Darling normality test to check the normality of the data.
```{r, echo = T, eval = T}
library(nortest)
ad.test(sample)
```
We get a p-value of $<2.2\cdot 10^{-16}$, which rejects the hypothesis that the data is not normal distributed, and we conclude that our algorithm is correct.

# Problem B: The gamma distribution

We can sample from the gamma distribution with $\alpha \in (0, 1)$ and $\beta = 1$,

$$ f(x) = \begin{cases}
\frac{1}{\Gamma(\alpha)}x^{\alpha-1}e^{-x}, \quad & 0<x,\\
0, \quad &\text{otherwise}. \\
\end{cases} $$

using the distribution found in A.2 as a proposal distribution. We can do this because the A.2 distribution is nonzero at all points where $f(x)$ is nonzero. 

```{r, echo = T, eval = T}
library(pracma)

x = linspace(0, 3, 100)
alpha = 0.5
y_gamma = dgamma(x, shape=alpha, rate=1)
y_prop = 1/(gamma(alpha)) * (1/alpha + exp(-1))*f_A2(x, alpha)

df10 = data.frame(x, y_gamma, y_prop)
h <- ggplot(data.frame(x, y_gamma, y_prop))
h = h + geom_line(aes(x = x, y = y_gamma, colour="Gamma"), size=1.)
h = h + geom_line(aes(x = x, y = y_prop, color="Proposal"), size=1.)
h = h + ggtitle(paste("Gamma(",toString(alpha),", 1) vs. proposal"))
h

```

We note that the acceptance probabiltiy, P_{accept}, 

$$P_{accept} = \int_0^{\infty} P_{unif}(U \leq \frac{f(x)}{c g(x)} ) P_{prop}(X = x) \\
 = \int_0^{\infty} \frac{f(x)}{c g(x)} g(x) = c^{-1},$$
 
To maximize the acceptance probability, we can minimize $c$ by choosing

$$ c = \underset{x}{\sup} \frac{f(x)}{g(x)} = \underset{x}{\sup} \begin{cases}
\frac{e^{-x}(\frac{1}{\alpha} + \frac{1}{e})}{\Gamma(\alpha)}, \quad 0 \leq x \le 1 \\
\frac{x^{\alpha - 1} (\frac{1}{\alpha} + \frac{1}{e})}{\Gamma(\alpha)}, \quad 1 \leq x
\end{cases}$$ 

Which means $c = \frac{(\frac{1}{\alpha} + \frac{1}{e})}{\Gamma(\alpha)}$

To generate $n$ samples from $f(x)$ we sample from $X \sim g(x)$ and $U \sim u[0, 1]$, and accept the sample $x$ if $u \leq \frac{f(x)}{c g(x)}$. We repeat the process until $n$ samples are accepted. 

We compare the sample mean and variance with the theoretical mean and variance and compare the sampled density with the theoretical density. 

```{r}
library(pracma)
c = 1
alpha = 0.5
rate = 1

sample_gamma = function(n=1000){
  uniform2 = runif(n)
  x = F_inv(n, alpha)
  c = 1/(gamma(alpha)) * (1/alpha + exp(-1))
  
  probs = dgamma(x, alpha, rate) / (c * f_A2(x, alpha))
  return(x[uniform2 <= probs])
}

g_samples <- sample_gamma(1000000)
g_samples_filt <- g_samples[g_samples < 5]
df12 = data.frame(x = g_samples_filt)

cat('Theoretical mean: ', alpha,' Computed mean: ', mean(g_samples),'\n')
cat('Theoretical variance: ', alpha, ' Computed variance: ', var(g_samples))

c = 1/(gamma(alpha)) * (1/alpha + exp(-1))

x_val = linspace(0, 5, 100)

df11 <- data.frame(x = x_val, y=dgamma(x_val, alpha, rate))

h <- ggplot()
h <- h + geom_line(data=df11, aes(x=x, y=y, color="Theoretical"), size=1.6)
h <- h + geom_histogram(data=df12, aes(x=x, y=..density.., color="Sampled"), binwidth=0.1, col = 'blue', fill = 'blue') 
h <- h + ggtitle("Comparison of analytical and sampled density")
h
```

As we remove the limitation on $\alpha$, we no longer get a closed form expression of $c$. To sample from the Gamma with parameters $\alpha > 1$, $\beta = 1$, we can use the ratio of uniforms method


